{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bc42730",
   "metadata": {},
   "source": [
    "### *Requirements*\n",
    "*To make use this notebook, you need an export of your search result from SolrWayback. (See **[SolrWayback > Export](https://nlnwa.github.io/research-services/docs/solrwayback/solrwayback-5export.html)** )*\n",
    "\n",
    "*The exported data:*\n",
    "- *must be in the JSONL format,*\n",
    "- *must contain the fields **'url_norm'** and **'crawl_date'** field.*\n",
    "- *should also contain the **'warc_key_id'** field.*\n",
    "\n",
    "*It is highly recommended that the number of exported results are below 20000. If your data is based on a search result with vast more hits, you should reduce the scope, e.g. by applying facets for specific domains or crawl year.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c89fb77",
   "metadata": {},
   "source": [
    "# Analyse Query Results: Detect Unique Urls vs Multiple Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e047bf54",
   "metadata": {},
   "source": [
    "Engaging with big data often necessitates initial analysis to establish some kind of sense of the data.\n",
    "\n",
    "This notebook will allow you to analyse how many of the resources in a SolrWayback query result that are have *unique urls*, and how many that are different *versions* of the same url.\n",
    "\n",
    "Different versions are not necessarily duplicates. The front page of a news site will often be very different from one day to the other. However, if you are using SolrWayback to define a corpus of resources you will examine, it is valuable to know if some of these resources are potentially \"overrepresented\" due to a high number of versions.\n",
    "\n",
    "This notebook let you analyse the number of unique urls in the corpus, and detect which archived urls that appears several times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c0dfe5",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86650a4",
   "metadata": {},
   "source": [
    "First, we need to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66919e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c2f209",
   "metadata": {},
   "source": [
    "## Load data from SolrWayback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8189132d",
   "metadata": {},
   "source": [
    "Then, we load the JSONL data set you exported from SolrWayback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5e895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../data/solrwayback_JonasGahrStore.jsonl', lines=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250f8e9b",
   "metadata": {},
   "source": [
    "### 1. Convert timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074811ba",
   "metadata": {},
   "source": [
    "The timestamps in our JSONL file is in the machine-readable Unix format. Before computing this, we want to convert it into python's Datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e48d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['crawl_date'] = pd.to_datetime(df['crawl_date'], unit='ms')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2f936d",
   "metadata": {},
   "source": [
    "## 1. Count resources (unique urls and urls with several versions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c371f4",
   "metadata": {},
   "source": [
    "Let us count the number each url appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0539a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_counts = df['url_norm'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eda6f7c",
   "metadata": {},
   "source": [
    "Then, we want to filter urls that appears more than one time in the data set.\n",
    "To make it easier to read, we limit the dataframe to the url, title and count columns.\n",
    "Then, we display a table with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1360f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_vs_version(df):\n",
    "    urls_with_multiple_timestamps = df.groupby('url_norm').filter(lambda x: x['crawl_date'].nunique() > 1)\n",
    "    urls_with_counts = urls_with_multiple_timestamps.groupby(['url_norm', 'title'])['crawl_date'].nunique().reset_index(name='versions')\n",
    "    urls_with_counts = urls_with_counts.sort_values(by='versions', ascending=False)\n",
    "    return urls_with_counts\n",
    "\n",
    "# Call the function to get urls_with_counts\n",
    "urls_with_counts = count_unique_vs_version(df)\n",
    "\n",
    "# Print summary message\n",
    "total_rows = df.shape[0]\n",
    "unique_urls = len(df['url_norm'].unique())\n",
    "multiple_versions = len(urls_with_counts)\n",
    "print(f\"Total number of lines in the dataset: \\033[1m{total_rows}\\033[0m \\nNumber of unique resources: \\033[1m{unique_urls}\\033[0m \\nNumber of resources archived in several versions: \\033[1m{multiple_versions}\\033[0m.\")\n",
    "\n",
    "display(urls_with_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778532c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
