{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ffb4121",
   "metadata": {},
   "source": [
    "### *Requirements*\n",
    "*To make use this notebook, you need an export of your search result from SolrWayback. (See **[SolrWayback > Export](https://nlnwa.github.io/research-services/docs/solrwayback/solrwayback-5export.html)** )*\n",
    "\n",
    "*The exported data:*\n",
    "- *must be in the JSONL format,*\n",
    "- *must contain the **'title'** field.*\n",
    "- *should also contain the fields **'warc_key_id'** and **domain**.*\n",
    "\n",
    "*It is highly recommended that the amount of exported results are below 20000. If your data is based on a search result with vast more hits, you should reduce the scope, e.g. by applying facets for specific domains or crawl year.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541a92b6",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Webpage Titles\n",
    "\n",
    "### What is sentiment analysis?\n",
    "Sentiment analysis is a computational technique to interpret and classify the emotional tone or connotations of a text.\n",
    "\n",
    "### Analysing sentiments of document titles\n",
    "This notebook will allow you to analyse the sentiments of document titles, exported from SolrWayback.\n",
    "\n",
    "It is based on a naive approach, using the [Norwegian Sentiment Lexicon](https://github.com/ltgoslo/norsentlex) from the Language Technology Group (LTG) at the Department of Informatics, University of Oslo.\n",
    "\n",
    "A sentiment lexicon is simply a list of potentially sentiment bearing words and their prior positive/negative polarity. This come with several shortcomings, since the polarity values are context-independent. However, the simplicity make it possible to run it with limited computer resources, and without training models for specific genres or domains.\n",
    "\n",
    "If sentiment analysis is a pivotal part of your methodology, and you have sufficient computational resources, you can consider training your own models or make use of pre-trained models for more fine-grained analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e2af16",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "Before starting, we must import the necessary python libraries.\n",
    "\n",
    "To run a code cell: Make sure it is marked and then press <kbd>Shift</kbd> + <kbd>Enter</kbd>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9bc876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617998eb",
   "metadata": {},
   "source": [
    "## Load data from SolrWayback into Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da1cb8",
   "metadata": {},
   "source": [
    "First, you need to load the data you exported from SolrWayback. You can change the name and path of your exported JSONL file in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3afa426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'your-file' below with the name of your file\n",
    "solrwb_corpus_titles = '../data/solrwayback_regjeringen-no.jsonl'\n",
    "\n",
    "# Reading the .jsonl file line by line into a list of dictionaries\n",
    "data_list = []\n",
    "\n",
    "with open(solrwb_corpus_titles, 'r') as f:\n",
    "    for line in f:\n",
    "        data_list.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8203801",
   "metadata": {},
   "source": [
    "Then, we read the data into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f03990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4794a9",
   "metadata": {},
   "source": [
    "Displaying the DataFrame allow us to see the name of the columns, and the values of the first and last 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613b70ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc2fc02",
   "metadata": {},
   "source": [
    "# Keeping only the needed columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653df2fc",
   "metadata": {},
   "source": [
    "Before processing, we want to remove rows where the title is missing (NaN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef67199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where the title is missing\n",
    "df = df.dropna(subset=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe01123",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8088a76",
   "metadata": {},
   "source": [
    "## Prepare classification of document title's sentiment\n",
    "\n",
    "### Loading sentiment lexica\n",
    "\n",
    "Now, you need to load the positive and negative lexica. (To speed up processing and reduce the look-up time, we loaded them into sets.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31d3574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lexica from text files\n",
    "positive_words = []\n",
    "negative_words = []\n",
    "\n",
    "with open('../resources/Fullform_Positive_lexicon.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        positive_words.append(line.strip())\n",
    "\n",
    "with open('../resources/Fullform_Negative_lexicon.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        negative_words.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c99875f",
   "metadata": {},
   "source": [
    "Then, you create the function that will perform the sentiment analysis.\n",
    "\n",
    "The first step will split the title of each document into single words.\n",
    "\n",
    "The machine will then look up each word in the positive and negative lexica. For each word found in the positive lexicon, it will add a sentiment score of +1, while each word found in the negative lexicon will subtract -1.\n",
    "\n",
    "If the sum of the words in the title above 0, the title will be classified as 'Positive'. If the sum is below 0, it will be classified as 'Negative'. If the sum is 0, the title will be classified as 'Neutral'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9d0a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis function\n",
    "def sentiment_analysis(title):\n",
    "    sentiment_score = 0\n",
    "    words = title.split()\n",
    "    \n",
    "    for word in words:\n",
    "        if word.lower() in positive_words:\n",
    "            sentiment_score += 1\n",
    "        elif word.lower() in negative_words:\n",
    "            sentiment_score -= 1\n",
    "    \n",
    "    if sentiment_score > 0:\n",
    "        return 'Positive'\n",
    "    elif sentiment_score < 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90468eaa",
   "metadata": {},
   "source": [
    "## Applying sentiment analysis to the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52108c5",
   "metadata": {},
   "source": [
    "Now, we are ready to perform the sentiment analysis. The code below will also output ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfac83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['title'].apply(sentiment_analysis)\n",
    "\n",
    "# Show DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa3dac",
   "metadata": {},
   "source": [
    "## Visualise sentiments in corpus\n",
    "After classifying the title's sentiments, we can visualise how "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e4d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentiment data from DataFrame\n",
    "sentiments = df['sentiment'].tolist()\n",
    "\n",
    "# Count the occurrences of each sentiment\n",
    "sentiment_counts = Counter(sentiments)\n",
    "\n",
    "# Visualize using Plotly\n",
    "fig = px.bar(x=list(sentiment_counts.keys()), y=list(sentiment_counts.values()), labels={'x':'Sentiment', 'y':'Count'}, title='Sentiment Distribution')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e156647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
