{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05de1daf-8e8d-4fda-9f94-ee14b50136fa",
   "metadata": {},
   "source": [
    "**Note:** *This notebook is a work in progress and serves as an example of how users can programmatically analyse the National Library of Norway’s collection of web news texts. The notebook allows you to:*\n",
    "\n",
    "- *build corpora and visualise distribution of titles, languages and harvest dates*  \n",
    "- *retrieve Keywords in Context (concordances),*  \n",
    "- *calculate the relative frequency of collocations for a keyword.*  \n",
    "\n",
    "*Do you have any questions? Feel free to read the [general information about the online newspaper corpus](https://www.nb.no/en/collection/web-archive/research/web-news-corpus/) or send us an email at [nettarkivet@nb.no](mailto:nettarkivet@nb.no).*\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09cfab3-b087-4c99-9d81-62964a58d2ca",
   "metadata": {},
   "source": [
    "# 0. Import `dhlab` for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4ba907-482a-41f8-8892-3458f78397a8",
   "metadata": {},
   "source": [
    "Before you begin, import the necessary Python packages. If you have not installed `dhlab` yet, you need to uncomment the first line by removing `# ` in front of `!pip install ...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a610b652-da43-4f84-b13c-77cb5ded0444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U dhlab\n",
    "import dhlab as dh\n",
    "import dhlab.nbtext as nb\n",
    "from dhlab import Corpus, totals, Collocations, Ngram\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pandas as df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb09be38-cbc6-4d51-9d2d-9344b25fc3f1",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb70b1a3-106c-42f2-b711-a626ffb5b5fa",
   "metadata": {},
   "source": [
    "# 1. Corpus Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1c9b6c-392b-4d21-9238-82f80c0167b2",
   "metadata": {},
   "source": [
    "In text analysis, a **[corpus](https://en.wikipedia.org/wiki/Text_corpus)** is a collection of texts. Below, you will be able to define corpora from the Web News Collection. In total, there are more than 1.5 million texts in the collection in various languages.\n",
    "\n",
    "In `dhlab`, the class `Corpus` can be used to represent metadata for each document. This includes metadata such as publication title, language, harvesting date, domain name, and more. Each text also has a unique `dhlabid`, which is DHlab’s persistent URN.\n",
    "\n",
    "If you want an overview of the various attributes of the texts as they are exposed through the API, you can read more about the [Web News Collection](https://www.nb.no/en/collection/web-archive/research/web-news-corpus/#which-schema-attributes-can-be-used-with-the-api?)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50169cc-7642-42c3-9026-c924f6b65566",
   "metadata": {},
   "source": [
    "## 1.1 Build a Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1041f2ea-6e94-464a-9de7-51272ca6848d",
   "metadata": {},
   "source": [
    "Let's build a corpus!\n",
    "\n",
    "The code cell below creates a corpus with texts containing either **covid-19** or **korona**.\n",
    "\n",
    "- `doctype=\"nettavis\"` specifies that we are working with web news.  \n",
    "- `fulltext=\"covid-19 OR korona\"` limits the scope to texts containing the specified keyword(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdc8f15-cd61-44ab-b4d8-4a9c3d45119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = dh.Corpus(doctype=\"nettavis\", fulltext=\"covid-19 OR korona\", limit=100000)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437127c1-e8ee-4c9b-a226-093fcd636dc0",
   "metadata": {},
   "source": [
    "## 1.2 Insight and Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5c34f7-93e7-497c-a3cf-a4b7d057b208",
   "metadata": {},
   "source": [
    "Before proceeding with text analysis, we might want some insight into the corpus that was built. Let's visualise how the texts are distributed by *publication*, *harvesting date* and *language*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d6911-be21-4328-a0a9-f8a7960437bf",
   "metadata": {},
   "source": [
    "### 1.2.1 Publication Titles (tree map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52a4972-b628-451f-8dfc-d66f8d2db27a",
   "metadata": {},
   "source": [
    "Displaying the distribution of texts per publication and visualise the distribution using a tree map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f816790-ffdc-419c-92d2-788da107e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_titles = corpus['title'].value_counts()\n",
    "\n",
    "fig = px.treemap(\n",
    "    path=[count_titles.index],\n",
    "    values=count_titles.values,\n",
    "    title='Distribution of texts per publication title',\n",
    "    hover_data={'Number of texts': count_titles.values}\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a1b48f-afbb-4aa2-a43e-5bfdceacdb68",
   "metadata": {},
   "source": [
    "### 1.2.2 Distribution over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd183f17-5bed-4791-91c6-8fc1a3af7802",
   "metadata": {},
   "source": [
    "Displaying the distribution of texts over time, based on harvest date. A small random offset (jitter) is applied to the x-axis (harvest date) to reduce overlap, while a random spread along the y-axis is introduced purely for visual separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d91e7f-fd22-4c66-b49c-19092773a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "harvestdate = df.to_datetime(corpus['timestamp'], format='%Y%m%d')\n",
    "\n",
    "# Generate random values for spread along the y-axis\n",
    "y_spread = np.random.uniform(0, 1, size=len(harvestdate))\n",
    "\n",
    "# Add jitter (small random offset) to the harvest dates\n",
    "x_jitter = harvestdate + df.to_timedelta(np.random.uniform(-2, 2, size=len(harvestdate)), unit='D')\n",
    "\n",
    "# Create a scatter plot with jitter for better distribution\n",
    "fig = px.scatter(\n",
    "    x=x_jitter,\n",
    "    y=y_spread,\n",
    "    labels={'x': 'Harvest Date', 'y': 'Spread'},\n",
    "    title='Distribution by Harvest Date',\n",
    "    opacity=0.5,  # Make points semi-transparent\n",
    "    size_max=5  # Increase point size\n",
    ")\n",
    "\n",
    "# Display the chart\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccd07a7-70fe-4adc-b9ce-37544fcd9caf",
   "metadata": {},
   "source": [
    "### 1.2.3 Distribution by Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8973cfde-fcfd-4bed-8a59-9588697d676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of text per language code\n",
    "language_distribution = corpus['langs'].value_counts()\n",
    "\n",
    "# Create pie chart\n",
    "fig = px.pie(\n",
    "    values=language_distribution.values,\n",
    "    names=language_distribution.index,\n",
    "    title='Distribution of Languages',\n",
    "    labels={'names': 'Language', 'values': 'Texts'}\n",
    ")\n",
    "\n",
    "# Display chart\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dbd2b8-9ada-4157-8b46-9440d0a6e842",
   "metadata": {},
   "source": [
    "## 1.3 Export Corpus Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c7f126-65b3-466e-b6aa-5ab3ff8da98a",
   "metadata": {},
   "source": [
    "If you are happy with the corpus you defined, you are almost ready to advance. But first, you probably want to export the corpus definition. This is convenient for research data management (RDM), enabling it possible for yourself and others to reproduce the dataset.\n",
    "\n",
    "You can export the corpus definiton as both Excel and JSONL. Set the filename below and run cells to export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608def7f-5d85-434a-b920-403fca31fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filename (without extension)\n",
    "filename = 'corpus-covid19-OR-korona'\n",
    "\n",
    "# Export Excel\n",
    "corpus.frame.to_excel(f\"{filename}.xlsx\", index=False)\n",
    "\n",
    "# Export JSONL\n",
    "corpus.frame.to_json(f\"{filename}.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee0f8d-f328-4ef2-b794-2e78e68b198a",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd768d9-d975-47ea-b2e2-8e435acd436a",
   "metadata": {},
   "source": [
    "## 2. Concordances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3354562-ee55-4a62-b3dd-35b2be0b003b",
   "metadata": {},
   "source": [
    "When you have defined your corpus, you can extract various types of information from it.\n",
    "\n",
    "One approach is by retreiving small snippets of text with **[Keyword in Context (KWIC)](https://en.wikipedia.org/wiki/Key_Word_in_Context)**, also known as *concordances*.\n",
    "\n",
    "The code cell below retrieves concordances, with a text window of up to 12 words before and after your keyword. The keyword is highlighted in bold. (To prevent that snippets can be used to reconstruct complete texts, snippets never crosses paragraphs.)\n",
    "\n",
    "Let’s request concordances for words starting with **\"dugnad\"** (see \"dugnad\" explained in [Wikipedia](https://en.wikipedia.org/wiki/Communal_work#Norway))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56b9211-dbea-4490-82f6-8696aeb4d3e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conc_dugnad = corpus.conc(words=\"dugnad*\")\n",
    "conc_dugnad.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cd054d-1743-40ef-be2e-298ad5897345",
   "metadata": {},
   "source": [
    "### Export concordances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d544153-e081-47f3-9a39-3dc9d489b15e",
   "metadata": {},
   "source": [
    "Once again, we can save the results as Excel and/or JSONL format. (Ensure to update the filename!)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c00ae65-d510-46ad-a81b-7cbebacd6e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filename (without extension)\n",
    "filename = 'concordances-dugnad'\n",
    "\n",
    "# Export Excel\n",
    "conc_dugnad.frame.to_excel(f\"{filename}.xlsx\", index=False)\n",
    "\n",
    "# Export JSONL\n",
    "conc_dugnad.frame.to_json(f\"{filename}.jsonl\", orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da390e0e-4153-4d74-b666-8b933889418e",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5996ff6b-8c0c-4cf5-9c58-7144e9bae006",
   "metadata": {},
   "source": [
    "## 3. Collocations and Word Frequencies\n",
    "\n",
    "**[Collocations](https://en.wikipedia.org/wiki/Collocation)** refer to word pairs that frequently occur together. By counting collocations for a given word, we can analyse how often or rarely different words co-occur.\n",
    "\n",
    "Some words tends to appear frequently in any text, such as `and`, `he`, and `she`. To identify words that are significant within a specific context, we calculate the relative frequency (RF) for collocates, comparing the frequency in our specific corpus with frequency in a general reference corpus.\n",
    "\n",
    "The cell below lists the words with the highest relative frequency, given a keyword in the corpus.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a6916f-a88f-46fd-b15c-fe4e21f9b7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = totals(75000)  # Fetches reference corpus with common word frequency\n",
    "coll = corpus.coll(\"dugnad\").frame.sort_values(by=\"counts\", ascending=False)  # Counts collocations for the keyword in the corpus\n",
    "(coll.counts / tot.freq * 100).sort_values(ascending=False).head(20)  # Calculates relative frequency (RF) in the corpus compared to the reference corpus\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
